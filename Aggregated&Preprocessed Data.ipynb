{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b151df4",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning\n",
    "### i.Checking for missing values\n",
    "### ii.Checking for duplicates\n",
    "### iii.Type conversion\n",
    "### iv.Handling Outliers\n",
    "### v.Consistency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7972acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c39cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset_Uber Traffic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bad7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime    0\n",
      "Junction    0\n",
      "Vehicles    0\n",
      "ID          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16f30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af958f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "#Checking for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "print(f'Number of duplicate rows: {duplicates.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d4084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37a44b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime    object\n",
      "Junction     int64\n",
      "Vehicles     int64\n",
      "ID           int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Ensuring ift he columns have appropriate data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a712a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type conversion\n",
    "# Converting 'DateTime' column to datetime type\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5891f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 3617\n"
     ]
    }
   ],
   "source": [
    "# Handling outliers using IQR method\n",
    "\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = df['Vehicles'].quantile(0.25)\n",
    "Q3 = df['Vehicles'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier thresholds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "outliers = df[(df['Vehicles'] < lower_bound) | (df['Vehicles'] > upper_bound)]\n",
    "print(f'Number of outliers: {outliers.shape[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34dccaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Outliers\n",
    "df_clean = df[(df['Vehicles'] >= lower_bound) & (df['Vehicles'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841bdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 44503\n",
      "Cleaned data size: 44503\n"
     ]
    }
   ],
   "source": [
    "# Verify if outliers are removed\n",
    "print(f'Original data size: {df.shape[0]}')\n",
    "print(f'Cleaned data size: {df_clean.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff6a65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No outliers detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "639805c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2015-01-11 00:00:00\n",
      "End Date: 2017-12-06 23:00:00\n",
      "[1 2 3 4]\n",
      "[20151101001 20151101011 20151101021 ... 20170630214 20170630224\n",
      " 20170630234]\n"
     ]
    }
   ],
   "source": [
    "# Checking the range of dates\n",
    "print(f'Start Date: {df[\"DateTime\"].min()}')\n",
    "print(f'End Date: {df[\"DateTime\"].max()}')\n",
    "\n",
    "# Check for consistency in 'Junction' and 'ID' columns\n",
    "print(df['Junction'].unique())\n",
    "print(df['ID'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53e5e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is consistent and covers a time span from January 11, 2015, to December 6, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64598d5e",
   "metadata": {},
   "source": [
    "### Data cleaning is done.\n",
    "### The dataset was already of good qualitity\n",
    "### There was little to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e3874",
   "metadata": {},
   "source": [
    "# 2. Aggregate traffic data\n",
    "### - Compile traffic data into hourly intervals for each junction.\n",
    "### - Ensure data includes relevant details such as vehicle counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25edd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling the data into hourly intervals with vehicle count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57a1df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Junction            DateTime  Vehicles\n",
      "0         1 2015-01-11 00:00:00        15\n",
      "1         1 2015-01-11 01:00:00        13\n",
      "2         1 2015-01-11 02:00:00        10\n",
      "3         1 2015-01-11 03:00:00         7\n",
      "4         1 2015-01-11 04:00:00         9\n"
     ]
    }
   ],
   "source": [
    "# Seting 'DateTime' as the index for resampling\n",
    "df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Resampling the data to hourly intervals and aggregate vehicle counts\n",
    "df_hourly = df.groupby('Junction').resample('H').agg({'Vehicles': 'sum'}).reset_index()\n",
    "\n",
    "# Check the first few rows of the aggregated data\n",
    "print(df_hourly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da5bce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's verify if there are any missing hours and data for a specific junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d458f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing hours: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing hours\n",
    "missing_hours = df_hourly[df_hourly['Vehicles'].isna()]\n",
    "print(f'Missing hours: {missing_hours.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f99ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Junction            DateTime  Vehicles\n",
      "76377         4 2017-01-01 00:00:00         3\n",
      "76378         4 2017-01-01 01:00:00         1\n",
      "76379         4 2017-01-01 02:00:00         4\n",
      "76380         4 2017-01-01 03:00:00         4\n",
      "76381         4 2017-01-01 04:00:00         2\n"
     ]
    }
   ],
   "source": [
    "# Checking the data for a specific junction\n",
    "junction_id = 4  # Change this to the junction of interest\n",
    "df_junction_hourly = df_hourly[df_hourly['Junction'] == junction_id]\n",
    "print(df_junction_hourly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4f19b",
   "metadata": {},
   "source": [
    "# 3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92f4343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Vehicles  HourOfDay  Junction_1  Junction_2  Junction_3  Junction_4\n",
      "0  0.254237   0.000000         1.0         0.0         0.0         0.0\n",
      "1  0.220339   0.043478         1.0         0.0         0.0         0.0\n",
      "2  0.169492   0.086957         1.0         0.0         0.0         0.0\n",
      "3  0.118644   0.130435         1.0         0.0         0.0         0.0\n",
      "4  0.152542   0.173913         1.0         0.0         0.0         0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "#Extracting HourOfDay\n",
    "df_hourly['HourOfDay'] = df_hourly['DateTime'].dt.hour\n",
    "\n",
    "# Define features\n",
    "features = ['Vehicles', 'Junction', 'HourOfDay']\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = ['Vehicles', 'HourOfDay']\n",
    "categorical_features = ['Junction']\n",
    "\n",
    "# Normalize numeric features\n",
    "scaler = MinMaxScaler()\n",
    "df_hourly[numeric_features] = scaler.fit_transform(df_hourly[numeric_features])\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_categorical = encoder.fit_transform(df_hourly[categorical_features])\n",
    "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Concatenate normalized numeric and encoded categorical features\n",
    "df_processed = pd.concat([df_hourly[numeric_features], encoded_categorical_df], axis=1)\n",
    "\n",
    "# Check the processed data\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0f14d",
   "metadata": {},
   "source": [
    "### Data is processed for comprisons accross different timeperiods and junctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd74e7",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering and Selection\n",
    "### i. TIme based features: hour of the day, day of the week, month\n",
    "### ii. lag features: including traffic data from previous hours or days to capture temporal dependencies.\n",
    "### iii. binary indicators for weekends and special events to account for their impact on traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31c80c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Extract time-based features\n",
    "df_hourly['HourOfDay'] = df_hourly['DateTime'].dt.hour\n",
    "df_hourly['DayOfWeek'] = df_hourly['DateTime'].dt.dayofweek\n",
    "df_hourly['Month'] = df_hourly['DateTime'].dt.month\n",
    "\n",
    "# Lag features\n",
    "df_hourly['PrevHourVehicles'] = df_hourly['Vehicles'].shift(1)\n",
    "df_hourly['PrevDayVehicles'] = df_hourly['Vehicles'].shift(24)  # assuming hourly data\n",
    "\n",
    "# Binary indicators\n",
    "df_hourly['IsWeekend'] = df_hourly['DateTime'].dt.dayofweek >= 5\n",
    "\n",
    "# Drop rows with NaN values generated by lag features\n",
    "df_hourly.dropna(inplace=True)\n",
    "\n",
    "# Feature and target columns\n",
    "features = ['HourOfDay', 'DayOfWeek', 'Month', 'PrevHourVehicles', 'PrevDayVehicles', 'IsWeekend']\n",
    "target = 'Vehicles'\n",
    "\n",
    "X = df_hourly[features]\n",
    "y = df_hourly[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f05334",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8606237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Feature  Importance\n",
      "3  PrevHourVehicles    0.828275\n",
      "0         HourOfDay    0.057449\n",
      "4   PrevDayVehicles    0.051924\n",
      "2             Month    0.034575\n",
      "1         DayOfWeek    0.024438\n",
      "5         IsWeekend    0.003340\n"
     ]
    }
   ],
   "source": [
    "# Evaluate feature importance using a RandomForest model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importance\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd56717",
   "metadata": {},
   "source": [
    "### It's done. Exploratory Data Analysis is done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
