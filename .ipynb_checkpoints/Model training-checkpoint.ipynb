{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8deb40c7",
   "metadata": {},
   "source": [
    " # Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51def12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              DateTime  Junction  Vehicles           ID            date_time  \\\n",
      "0  2015-01-11 00:00:00         1        15  20151101001  2009-01-11 00:00:00   \n",
      "1  2015-01-11 00:00:00         1        15  20151101001  2010-01-11 00:00:00   \n",
      "2  2015-01-11 00:00:00         1        15  20151101001  2011-01-11 00:00:00   \n",
      "3  2015-01-11 00:00:00         1        15  20151101001  2012-01-11 00:00:00   \n",
      "4  2015-01-11 00:00:00         1        15  20151101001  2013-01-11 00:00:00   \n",
      "\n",
      "   maxtempC  mintempC  totalSnow_cm  sunHour  uvIndex  ...  precipMM  \\\n",
      "0      27.0      15.0           0.0     11.6      6.0  ...       0.0   \n",
      "1      26.0      17.0           0.0     11.6      5.0  ...       0.0   \n",
      "2      28.0      14.0           0.0     11.6      5.0  ...       0.0   \n",
      "3      29.0      17.0           0.0     11.6      5.0  ...       0.0   \n",
      "4      29.0      16.0           0.0     11.6      6.0  ...       0.0   \n",
      "\n",
      "   pressure     tempC visibility winddirDegree windspeedKmph  date  day  \\\n",
      "0    1016.0 -1.250127       10.0         100.0     -0.654230   NaN  NaN   \n",
      "1    1015.0 -0.637934        2.0         130.0     -1.136464   NaN  NaN   \n",
      "2    1012.0 -1.862320       10.0          32.0     -0.895347   NaN  NaN   \n",
      "3    1014.0 -0.637934       10.0         193.0     -1.859814   NaN  NaN   \n",
      "4    1014.0 -1.250127       10.0          93.0     -0.171996   NaN  NaN   \n",
      "\n",
      "   holiday  holiday_type  \n",
      "0      NaN           NaN  \n",
      "1      NaN           NaN  \n",
      "2      NaN           NaN  \n",
      "3      NaN           NaN  \n",
      "4      NaN           NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the combined dataset\n",
    "combined_data = pd.read_csv('combined_data.csv')\n",
    "\n",
    "print(combined_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee794a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92040 entries, 0 to 92039\n",
      "Data columns (total 33 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   DateTime           92040 non-null  datetime64[ns]\n",
      " 1   Junction           92040 non-null  int64         \n",
      " 2   Vehicles           92040 non-null  int64         \n",
      " 3   ID                 92040 non-null  int64         \n",
      " 4   date_time          92040 non-null  datetime64[ns]\n",
      " 5   maxtempC           92040 non-null  float64       \n",
      " 6   mintempC           92040 non-null  float64       \n",
      " 7   totalSnow_cm       92040 non-null  float64       \n",
      " 8   sunHour            92040 non-null  float64       \n",
      " 9   uvIndex            92040 non-null  float64       \n",
      " 10  uvIndex.1          92040 non-null  float64       \n",
      " 11  moon_illumination  92040 non-null  float64       \n",
      " 12  moonrise           92040 non-null  object        \n",
      " 13  moonset            92040 non-null  object        \n",
      " 14  sunrise            92040 non-null  object        \n",
      " 15  sunset             92040 non-null  object        \n",
      " 16  DewPointC          92040 non-null  float64       \n",
      " 17  FeelsLikeC         92040 non-null  float64       \n",
      " 18  HeatIndexC         92040 non-null  float64       \n",
      " 19  WindChillC         92040 non-null  float64       \n",
      " 20  WindGustKmph       92040 non-null  float64       \n",
      " 21  cloudcover         92040 non-null  float64       \n",
      " 22  humidity           92040 non-null  float64       \n",
      " 23  precipMM           92040 non-null  float64       \n",
      " 24  pressure           92040 non-null  float64       \n",
      " 25  tempC              92040 non-null  float64       \n",
      " 26  visibility         92040 non-null  float64       \n",
      " 27  winddirDegree      92040 non-null  float64       \n",
      " 28  windspeedKmph      92040 non-null  float64       \n",
      " 29  date               92040 non-null  datetime64[ns]\n",
      " 30  day                88872 non-null  object        \n",
      " 31  holiday            88872 non-null  object        \n",
      " 32  holiday_type       88872 non-null  object        \n",
      "dtypes: datetime64[ns](3), float64(20), int64(3), object(7)\n",
      "memory usage: 23.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Converting all DateTime columns to datetime type\n",
    "combined_data['DateTime'] = pd.to_datetime(combined_data['DateTime'])\n",
    "combined_data['date_time'] = pd.to_datetime(combined_data['DateTime'])\n",
    "combined_data['date'] = pd.to_datetime(combined_data['DateTime'])\n",
    "\n",
    "print(combined_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61a1c0",
   "metadata": {},
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdfeb7",
   "metadata": {},
   "source": [
    "### training and validation sets based on the DateTime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3576bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2015-01-11 00:00:00\n",
      "End Date: 2017-12-06 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Check the starting and ending dates\n",
    "start_date = combined_data['DateTime'].min()\n",
    "end_date = combined_data['DateTime'].max()\n",
    "\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"End Date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665b6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (91464, 33)\n",
      "Validation data shape: (576, 33)\n"
     ]
    }
   ],
   "source": [
    "start_date = pd.Timestamp('2015-01-11 00:00:00')\n",
    "# Setting end_date a few days before the max_date for validation\n",
    "end_date = pd.Timestamp('2017-12-01 00:00:00')  # Adjusted end_date for splitting\n",
    "\n",
    "# Creating training and validation datasets\n",
    "train_data = combined_data[(combined_data['DateTime'] >= start_date) & (combined_data['DateTime'] < end_date)]\n",
    "validation_data = combined_data[combined_data['DateTime'] >= end_date]\n",
    "\n",
    "# Checking the shapes\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c39f7",
   "metadata": {},
   "source": [
    "# Arima Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964ee7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pmdarima in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (1.2.0)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (1.10.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (0.14.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (1.26.16)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (68.0.0)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pmdarima) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=0.19->pmdarima) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->pmdarima) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pmdarima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139eef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Find the best ARIMA parameters automatically\n",
    "auto_model = auto_arima(train_data['Vehicles'], seasonal=False, trace=True)\n",
    "\n",
    "# Extract the optimal parameters\n",
    "p, d, q = auto_model.order\n",
    "print(f'Optimal ARIMA parameters: p={p}, d={d}, q={q}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Define the ARIMA model with the optimal parameters\n",
    "model = ARIMA(train_data['Vehicles'], order=(5, 1, 2))\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b8dfc",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(validation_data) - 1, dynamic=False)\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, index=validation_data.index, columns=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6858b9",
   "metadata": {},
   "source": [
    "# Evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "target_column = 'Vehicles'\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(validation_data[target_column], predictions)\n",
    "rmse = np.sqrt(mean_squared_error(validation_data[target_column], predictions))\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11dace",
   "metadata": {},
   "source": [
    "### Arima model is trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f9d4c",
   "metadata": {},
   "source": [
    "# Training LSTM (Long Short-Term Memory) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea57b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_data[[target_column]])\n",
    "val_scaled = scaler.transform(validation_data[[target_column]])\n",
    "\n",
    "# Convert data to sequences for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 10  # Example sequence length\n",
    "X_train, y_train = create_sequences(train_scaled, seq_length)\n",
    "X_val, y_val = create_sequences(val_scaled, seq_length)\n",
    "\n",
    "# Reshape the data to fit the LSTM input\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6744e",
   "metadata": {},
   "source": [
    "\n",
    "### training LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd224cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(seq_length, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28710df4",
   "metadata": {},
   "source": [
    "\n",
    "### Making Predictions with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "lstm_predictions = model.predict(X_val)\n",
    "\n",
    "# Inverse transform the predictions and actual values\n",
    "lstm_predictions = scaler.inverse_transform(lstm_predictions)\n",
    "y_val = scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "lstm_mae = mean_absolute_error(y_val, lstm_predictions)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_val, lstm_predictions))\n",
    "\n",
    "print(f'LSTM Mean Absolute Error (MAE): {lstm_mae}')\n",
    "print(f'LSTM Root Mean Squared Error (RMSE): {lstm_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce46c0",
   "metadata": {},
   "source": [
    "# Training Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54985493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# No need to reshape the data for tree-based models\n",
    "X_train = train_data.drop(columns=[target_column])\n",
    "y_train = train_data[target_column]\n",
    "X_val = validation_data.drop(columns=[target_column])\n",
    "y_val = validation_data[target_column]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859df97",
   "metadata": {},
   "source": [
    "### Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)  # Check feature data types\n",
    "print(y_train.dtypes)  # Check target data type\n",
    "\n",
    "# Example for validation set\n",
    "print(X_val.dtypes)\n",
    "print(y_val.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73488a7a",
   "metadata": {},
   "source": [
    "###  Convert DateTime Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DateTime column to numeric features\n",
    "X_train['DateTime'] = pd.to_datetime(X_train['DateTime'])\n",
    "X_val['DateTime'] = pd.to_datetime(X_val['DateTime'])\n",
    "\n",
    "X_train['Year'] = X_train['DateTime'].dt.year\n",
    "X_train['Month'] = X_train['DateTime'].dt.month\n",
    "X_train['Day'] = X_train['DateTime'].dt.day\n",
    "X_train['Hour'] = X_train['DateTime'].dt.hour\n",
    "\n",
    "X_val['Year'] = X_val['DateTime'].dt.year\n",
    "X_val['Month'] = X_val['DateTime'].dt.month\n",
    "X_val['Day'] = X_val['DateTime'].dt.day\n",
    "X_val['Hour'] = X_val['DateTime'].dt.hour\n",
    "\n",
    "# Drop the original DateTime column if not needed\n",
    "X_train = X_train.drop(columns=['DateTime'])\n",
    "X_val = X_val.drop(columns=['DateTime'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d834210",
   "metadata": {},
   "source": [
    "### Convert Time Strings to Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def time_to_numeric(time_str):\n",
    "    try:\n",
    "        # Convert time string to datetime object\n",
    "        time_obj = datetime.strptime(time_str, '%I:%M %p')\n",
    "        return time_obj.hour + time_obj.minute / 60.0  # Convert to decimal hours\n",
    "    except ValueError:\n",
    "        return np.nan  # Handle cases where time_str might be invalid or missing\n",
    "\n",
    "# Apply the conversion to time columns\n",
    "X_train['moonrise_numeric'] = X_train['moonrise'].apply(time_to_numeric)\n",
    "X_train['moonset_numeric'] = X_train['moonset'].apply(time_to_numeric)\n",
    "X_train['sunrise_numeric'] = X_train['sunrise'].apply(time_to_numeric)\n",
    "X_train['sunset_numeric'] = X_train['sunset'].apply(time_to_numeric)\n",
    "\n",
    "X_val['moonrise_numeric'] = X_val['moonrise'].apply(time_to_numeric)\n",
    "X_val['moonset_numeric'] = X_val['moonset'].apply(time_to_numeric)\n",
    "X_val['sunrise_numeric'] = X_val['sunrise'].apply(time_to_numeric)\n",
    "X_val['sunset_numeric'] = X_val['sunset'].apply(time_to_numeric)\n",
    "\n",
    "# Drop the original time columns if no longer needed\n",
    "X_train = X_train.drop(columns=['moonrise', 'moonset', 'sunrise', 'sunset'])\n",
    "X_val = X_val.drop(columns=['moonrise', 'moonset', 'sunrise', 'sunset'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b1205",
   "metadata": {},
   "source": [
    "### Apply One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to categorical columns\n",
    "X_train = pd.get_dummies(X_train, columns=['day', 'holiday', 'holiday_type'])\n",
    "X_val = pd.get_dummies(X_val, columns=['day', 'holiday', 'holiday_type'])\n",
    "\n",
    "# Ensure that the same columns are present in both train and validation sets\n",
    "X_train, X_val = X_train.align(X_val, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Define and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9c831",
   "metadata": {},
   "source": [
    "### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training and validation sets\n",
    "print(X_train.isna().sum())\n",
    "print(X_val.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef6105",
   "metadata": {},
   "source": [
    "###  Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd338e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object with a strategy (mean, median, etc.)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the imputer on training data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data\n",
    "X_val_imputed = imputer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513df7d",
   "metadata": {},
   "source": [
    "### Dropping Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "X_train_dropped = X_train.dropna()\n",
    "y_train_dropped = y_train[X_train_dropped.index]  # Ensure target variable is aligned\n",
    "\n",
    "X_val_dropped = X_val.dropna()\n",
    "y_val_dropped = y_val[X_val_dropped.index]  # Ensure target variable is aligned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6df480",
   "metadata": {},
   "source": [
    "### Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed3bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Train the model with imputed data\n",
    "gbr.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "gbr_predictions = gbr.predict(X_val_imputed)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "gbr_mae = mean_absolute_error(y_val, gbr_predictions)\n",
    "gbr_rmse = np.sqrt(mean_squared_error(y_val, gbr_predictions))\n",
    "\n",
    "print(f'Gradient Boosting Trees Mean Absolute Error (MAE): {gbr_mae}')\n",
    "print(f'Gradient Boosting Trees Root Mean Squared Error (RMSE): {gbr_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48ead5",
   "metadata": {},
   "source": [
    "# ARIMA Model:\n",
    "\n",
    "### MAE: 17.75\n",
    "### RMSE: 29.20\n",
    "\n",
    "# LSTM Model:\n",
    "\n",
    "### MAE: 0.01\n",
    "### RMSE: 0.03\n",
    "\n",
    "# Gradient Boosting Trees Model:\n",
    "\n",
    "### MAE: 6.76\n",
    "### RMSE: 9.94"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed96acb",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "### LSTM Model: Shows the lowest MAE and RMSE, indicating the best performance among the models tested.\n",
    "### Gradient Boosting Trees: Offers better performance compared to ARIMA but is not as accurate as LSTM.\n",
    "### ARIMA Model: Has the highest MAE and RMSE, suggesting itâ€™s less effective compared to the other models for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34e72d",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for LSTM\n",
    "### using a grid search to find the optimal configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4002f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "class LSTMHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50), \n",
    "                       return_sequences=True, \n",
    "                       input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "                      loss='mean_squared_error')\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a50fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras_tuner import HyperModel\n",
    "\n",
    "class LSTMHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50), \n",
    "                                       return_sequences=True, \n",
    "                                       input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50)))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "                      loss='mean_squared_error')\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f710f",
   "metadata": {},
   "source": [
    "### Checking data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dae9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c498aa5",
   "metadata": {},
   "source": [
    "### Reshaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reshape_for_lstm(data, time_steps):\n",
    "    samples = data.shape[0] // time_steps\n",
    "    features = data.shape[1]\n",
    "    reshaped_data = data[:samples * time_steps].reshape((samples, time_steps, features))\n",
    "    return reshaped_data\n",
    "\n",
    "# Define the time steps you want to use\n",
    "time_steps = 10\n",
    "\n",
    "# Reshape the training and validation data\n",
    "X_train_reshaped = reshape_for_lstm(X_train.values, time_steps)\n",
    "X_val_reshaped = reshape_for_lstm(X_val.values, time_steps)\n",
    "\n",
    "# Check the new shapes\n",
    "print(X_train_reshaped.shape)\n",
    "print(X_val_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d90c1",
   "metadata": {},
   "source": [
    "### Adjusting Model Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6318dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50), \n",
    "                                       return_sequences=True, \n",
    "                                       input_shape=(time_steps, X_train_reshaped.shape[2])))\n",
    "        model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50)))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "                      loss='mean_squared_error')\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc8755",
   "metadata": {},
   "source": [
    "### Defining the HyperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "class LSTMHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(\n",
    "            units=hp.Int('units', min_value=50, max_value=200, step=50),\n",
    "            return_sequences=True,\n",
    "            input_shape=(time_steps, X_train_reshaped.shape[2])\n",
    "        ))\n",
    "        model.add(tf.keras.layers.LSTM(\n",
    "            units=hp.Int('units', min_value=50, max_value=200, step=50)\n",
    "        ))\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9196118",
   "metadata": {},
   "source": [
    "### Creating the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8858419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch\n",
    "\n",
    "hypermodel = LSTMHyperModel()\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68318007",
   "metadata": {},
   "source": [
    "### Handling NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN or Inf values in the training and validation data\n",
    "print(np.any(np.isnan(X_train_reshaped)))\n",
    "print(np.any(np.isnan(y_train[:X_train_reshaped.shape[0]])))\n",
    "print(np.any(np.isnan(X_val_reshaped)))\n",
    "print(np.any(np.isnan(y_val[:X_val_reshaped.shape[0]])))\n",
    "\n",
    "print(np.any(np.isinf(X_train_reshaped)))\n",
    "print(np.any(np.isinf(y_train[:X_train_reshaped.shape[0]])))\n",
    "print(np.any(np.isinf(X_val_reshaped)))\n",
    "print(np.any(np.isinf(y_val[:X_val_reshaped.shape[0]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6841993",
   "metadata": {},
   "source": [
    "## Reshaping Data\n",
    "### Flattening the 3D Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05285678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Flatten the 3D arrays to 2D arrays\n",
    "X_train_flattened = X_train_reshaped.reshape(-1, X_train_reshaped.shape[-1])\n",
    "X_val_flattened = X_val_reshaped.reshape(-1, X_val_reshaped.shape[-1])\n",
    "\n",
    "# Create an imputer object with strategy 'mean'\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_imputed = imputer.fit_transform(X_train_flattened)\n",
    "\n",
    "# Transform the validation data\n",
    "X_val_imputed = imputer.transform(X_val_flattened)\n",
    "\n",
    "# Reshape back to original 3D dimensions\n",
    "X_train_reshaped = X_train_imputed.reshape(X_train_reshaped.shape)\n",
    "X_val_reshaped = X_val_imputed.reshape(X_val_reshaped.shape)\n",
    "\n",
    "# Check for NaN values to confirm imputation\n",
    "print(np.any(np.isnan(X_train_reshaped)))\n",
    "print(np.any(np.isnan(X_val_reshaped)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192e7f3",
   "metadata": {},
   "source": [
    "###  Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming the data is 3D (samples, time_steps, features)\n",
    "n_samples, n_time_steps, n_features = X_train_reshaped.shape\n",
    "\n",
    "# Flatten the data for scaling\n",
    "X_train_flattened = X_train_reshaped.reshape(-1, n_features)\n",
    "X_val_flattened = X_val_reshaped.reshape(-1, n_features)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_val_scaled = scaler.transform(X_val_flattened)\n",
    "\n",
    "# Reshape back to original shape\n",
    "X_train_reshaped = X_train_scaled.reshape(n_samples, n_time_steps, n_features)\n",
    "X_val_reshaped = X_val_scaled.reshape(X_val_reshaped.shape[0], n_time_steps, n_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c4c1a",
   "metadata": {},
   "source": [
    "### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b991aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def create_lstm_model(units=50, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b256f44",
   "metadata": {},
   "source": [
    "###  Dropout Layers\n",
    "### Adding dropout layers can help prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e508a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def create_lstm_model_with_dropout(units=50, optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
    "        LSTM(units, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31871ae4",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def create_simple_lstm_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = create_simple_lstm_model()\n",
    "history = model.fit(X_train_reshaped, y_train[:X_train_reshaped.shape[0]],\n",
    "                    epochs=10, validation_data=(X_val_reshaped, y_val[:X_val_reshaped.shape[0]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae68e5",
   "metadata": {},
   "source": [
    "# Model Development and Training is done......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca36801",
   "metadata": {},
   "source": [
    "# Model Evaluation and Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b21f9",
   "metadata": {},
   "source": [
    "### Verifying and Adjusting Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many complete samples fit into the reshaping dimensions\n",
    "num_complete_samples = X_val.shape[0] // 10  # Number of complete samples with 10 timesteps\n",
    "\n",
    "# Adjust X_val to have a shape that fits (num_complete_samples, 10, 43)\n",
    "X_val_correct = X_val[:num_complete_samples * 10].reshape(num_complete_samples, 10, 43)\n",
    "\n",
    "# Predict using the corrected validation data\n",
    "y_pred = model.predict(X_val_correct).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b61292",
   "metadata": {},
   "source": [
    "### Ensuring y_val Matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust y_val to match the number of complete samples\n",
    "y_val_correct = y_val[:num_complete_samples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ae627",
   "metadata": {},
   "source": [
    "# Calculate Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34116bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lengths before calculating metrics\n",
    "assert len(y_pred) == len(y_val_correct), \"Mismatch in prediction and validation labels lengths\"\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_val_correct, y_pred)\n",
    "rmse = mean_squared_error(y_val_correct, y_pred, squared=False)\n",
    "r2 = r2_score(y_val_correct, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6206a",
   "metadata": {},
   "source": [
    "# Visualization for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ff6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb8488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e2876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
